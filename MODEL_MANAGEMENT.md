# ğŸ¤– Model Management System

Há»‡ thá»‘ng quáº£n lÃ½ models Ä‘á»™c láº­p cho ChatBot vá»›i auto-detection, auto-download vÃ  production deployment.

## ğŸ“‹ Tá»•ng quan

Há»‡ thá»‘ng Model Management Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ:

- âœ… **TÃ¡ch biá»‡t cÃ¡c model** hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p
- âœ… **Auto-detect** models cÃ³ sáºµn trÃªn mÃ¡y
- âœ… **Auto-download** models thiáº¿u
- âœ… **Há»— trá»£ production** deployment
- âœ… **Quáº£n lÃ½ qua API** vÃ  CLI
- âœ… **Validation** vÃ  health check

## ğŸ—ï¸ Kiáº¿n trÃºc

```
ModelManager
â”œâ”€â”€ Model Detection
â”‚   â”œâ”€â”€ Ollama Models
â”‚   â”œâ”€â”€ Vosk Models  
â”‚   â”œâ”€â”€ Local Models
â”‚   â””â”€â”€ GitHub Models
â”œâ”€â”€ Model Download
â”‚   â”œâ”€â”€ Concurrent Downloads
â”‚   â”œâ”€â”€ Progress Tracking
â”‚   â””â”€â”€ Error Handling
â”œâ”€â”€ Model Validation
â”‚   â”œâ”€â”€ Health Checks
â”‚   â”œâ”€â”€ Performance Tests
â”‚   â””â”€â”€ Integrity Verification
â””â”€â”€ Model Management
    â”œâ”€â”€ Status Monitoring
    â”œâ”€â”€ Cleanup Operations
    â””â”€â”€ Configuration Management
```

## ğŸš€ Quick Start

### 1. Kiá»ƒm tra status models
```bash
# Via API
curl http://localhost:8000/models/status

# Via CLI
python backend/scripts/model_cli.py status
```

### 2. Download required models
```bash
# Auto download required models
curl -X POST http://localhost:8000/models/ensure

# Download specific model
curl -X POST http://localhost:8000/models/download/ollama:gemma2:9b

# Via CLI
python backend/scripts/model_cli.py download
python backend/scripts/model_cli.py download --models ollama:gemma2:9b vosk:vi
```

### 3. Validate models
```bash
# Via API
curl -X POST http://localhost:8000/models/validate

# Via CLI
python backend/scripts/model_cli.py validate
```

## ğŸ“Š API Endpoints

### Model Status
```http
GET /models/status
```
Tráº£ vá» status cá»§a táº¥t cáº£ models:
```json
{
  "success": true,
  "data": {
    "total_models": 5,
    "available_models": 4,
    "required_models": 2,
    "missing_required": 0,
    "models_by_type": {
      "ollama": [...],
      "vosk": [...],
      "github": [...]
    },
    "model_details": {...}
  }
}
```

### Detect Models
```http
GET /models/detect
```
Detect táº¥t cáº£ models cÃ³ sáºµn trÃªn há»‡ thá»‘ng.

### Download Models
```http
POST /models/ensure
```
Download táº¥t cáº£ required models (background task).

```http
POST /models/download/{model_key}
```
Download má»™t model cá»¥ thá»ƒ.

### Validate Models
```http
POST /models/validate
```
Validate táº¥t cáº£ models vÃ  tráº£ vá» káº¿t quáº£.

### Cleanup Models
```http
POST /models/cleanup?keep_recent=3
```
Dá»n dáº¹p models cÅ©, giá»¯ láº¡i `keep_recent` models má»›i nháº¥t.

### Configure Models
```http
POST /models/configure
Content-Type: application/json

{
  "OLLAMA_MODEL": "gemma2:9b",
  "PREFERRED_AI_PROVIDER": "ollama",
  "AUTO_DOWNLOAD_MODELS": true
}
```

### Health Check
```http
GET /models/health
```
Health check cho model management system.

## ğŸ› ï¸ CLI Commands

### Status
```bash
python backend/scripts/model_cli.py status
```

### Detect
```bash
python backend/scripts/model_cli.py detect
```

### Download
```bash
# Download required models
python backend/scripts/model_cli.py download

# Download specific models
python backend/scripts/model_cli.py download --models ollama:gemma2:9b vosk:vi
```

### Validate
```bash
python backend/scripts/model_cli.py validate
```

### Cleanup
```bash
# Cleanup with default (keep 3 recent)
python backend/scripts/model_cli.py cleanup

# Cleanup keeping 5 recent models
python backend/scripts/model_cli.py cleanup --keep 5
```

### Recommended Models
```bash
python backend/scripts/model_cli.py recommended
```

### Configuration
```bash
python backend/scripts/model_cli.py configure
```

## ğŸ“¦ Supported Models

### Ollama Models
| Model | Size | Description | Recommended For |
|-------|------|-------------|-----------------|
| `gemma2:9b` | 5GB | Model máº·c Ä‘á»‹nh, cÃ¢n báº±ng | General purpose, Vietnamese |
| `llama3.1:8b` | 4.5GB | Model phá»• biáº¿n, nhanh | Fast responses, English |
| `qwen2.5:7b` | 4GB | Tá»‘t cho tiáº¿ng Viá»‡t | Vietnamese, multilingual |
| `gemma2:2b` | 1.5GB | Model nháº¹ | Low resource servers |

### Vosk Models
| Model | Size | Description | Required |
|-------|------|-------------|----------|
| `vosk-vi` | 78MB | Vietnamese speech recognition | âœ… Yes |
| `vosk-en` | 40MB | English speech recognition | âŒ No |

### GitHub Models
| Model | Size | Description | Required |
|-------|------|-------------|----------|
| `openai/gpt-4o-mini` | Unknown | GitHub AI model | âœ… Yes (if API key) |

## âš™ï¸ Configuration

### Environment Variables
```bash
# Model Management
AUTO_SETUP_ENABLED=true
DOWNLOAD_MODELS_ON_START=true
MAX_CONCURRENT_DOWNLOADS=2
MODEL_DOWNLOAD_TIMEOUT=1800

# AI Configuration
PREFERRED_AI_PROVIDER=ollama
OLLAMA_MODEL=gemma2:9b
OLLAMA_BASE_URL=http://localhost:11434

# Vosk Configuration
VOSK_MODEL_PATH=/app/data/models/vosk-vi
```

### Config.json
```json
{
  "OLLAMA_BASE_URL": "http://localhost:11434",
  "OLLAMA_MODEL": "gemma2:9b",
  "VOSK_MODEL_PATH": "/app/data/models/vosk-vi",
  "PREFERRED_AI_PROVIDER": "ollama",
  "AUTO_DOWNLOAD_MODELS": true,
  "MODEL_DOWNLOAD_TIMEOUT": 1800,
  "MAX_CONCURRENT_DOWNLOADS": 2
}
```

## ğŸš€ Production Deployment

### 1. Pre-deployment Setup
```bash
# Check prerequisites
./scripts/deploy_production.sh

# Setup production environment
docker-compose -f docker-compose.prod.yml up -d
```

### 2. Model Management in Production
```bash
# Check model status
curl https://your-domain.com/models/status

# Ensure required models
curl -X POST https://your-domain.com/models/ensure

# Monitor model health
curl https://your-domain.com/models/health
```

### 3. Monitoring
```bash
# Use monitoring script
./monitor.sh

# Check logs
docker-compose -f docker-compose.prod.yml logs -f backend
```

### 4. Backup
```bash
# Manual backup
./backup.sh

# Automatic backup (daily at 2 AM)
# Configured via cron job
```

## ğŸ”§ Troubleshooting

### Models Not Downloading
```bash
# Check disk space
df -h

# Check network connectivity
ping registry.ollama.ai

# Check Ollama server
curl http://localhost:11434/api/tags

# Check logs
docker-compose logs backend | grep -i model
```

### Model Validation Failed
```bash
# Check model integrity
python backend/scripts/model_cli.py validate

# Re-download corrupted models
python backend/scripts/model_cli.py download --models ollama:gemma2:9b

# Check Ollama server status
curl http://localhost:11434/api/tags
```

### Performance Issues
```bash
# Check resource usage
docker stats

# Cleanup old models
python backend/scripts/model_cli.py cleanup --keep 2

# Monitor model performance
curl http://localhost:8000/models/status
```

## ğŸ“ˆ Monitoring & Metrics

### Health Checks
- Model availability status
- Download progress tracking
- Validation results
- Performance metrics

### Logs
```bash
# Model management logs
docker-compose logs backend | grep -i model

# Download progress
docker-compose logs backend | grep -i download

# Validation results
docker-compose logs backend | grep -i validate
```

### Metrics
- Total models count
- Available models count
- Missing required models
- Download success rate
- Validation success rate

## ğŸ”’ Security Considerations

### Model Security
- Models downloaded from trusted sources only
- Checksum verification (planned)
- Model integrity validation
- Secure storage in Docker volumes

### API Security
- Rate limiting on download endpoints
- Authentication for configuration changes
- Input validation for model keys
- Error handling without sensitive data exposure

### Network Security
- HTTPS for production deployments
- Firewall rules for model downloads
- Proxy configuration support
- Network isolation for model containers

## ğŸš€ Future Enhancements

### Planned Features
- [ ] Model versioning and rollback
- [ ] Model performance benchmarking
- [ ] Automatic model updates
- [ ] Model sharing between instances
- [ ] Advanced caching strategies
- [ ] Model compression and optimization

### Integration Ideas
- [ ] Kubernetes operator for model management
- [ ] Prometheus metrics integration
- [ ] Grafana dashboards
- [ ] CI/CD pipeline integration
- [ ] Multi-cloud model distribution

## ğŸ“š References

- [Ollama Documentation](https://ollama.ai/docs)
- [Vosk Documentation](https://alphacephei.com/vosk/)
- [GitHub AI Models](https://models.github.ai/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)

---

**Note**: Há»‡ thá»‘ng Model Management Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ hoáº¡t Ä‘á»™ng Ä‘á»™c láº­p vÃ  cÃ³ thá»ƒ má»Ÿ rá»™ng. Táº¥t cáº£ models Ä‘Æ°á»£c quáº£n lÃ½ tá»± Ä‘á»™ng vÃ  cÃ³ thá»ƒ Ä‘Æ°á»£c monitor qua API hoáº·c CLI tools.


