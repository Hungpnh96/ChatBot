#!/usr/bin/env python3
"""
Script Ä‘á»ƒ cÃ i Ä‘áº·t cáº£ 2 models: gemma3n:e4b vÃ  gemma2:2b
"""

import requests
import json
import time
import sys
import os

def check_ollama_server():
    """Kiá»ƒm tra Ollama server cÃ³ cháº¡y khÃ´ng"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def get_installed_models():
    """Láº¥y danh sÃ¡ch models Ä‘Ã£ cÃ i Ä‘áº·t"""
    try:
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code == 200:
            data = response.json()
            return [model['name'] for model in data.get('models', [])]
        return []
    except:
        return []

def install_model(model_name):
    """CÃ i Ä‘áº·t model"""
    print(f"Äang cÃ i Ä‘áº·t model {model_name}...")
    
    try:
        # Gá»i API Ä‘á»ƒ pull model
        response = requests.post(
            "http://localhost:11434/api/pull",
            json={"name": model_name},
            stream=True
        )
        
        if response.status_code == 200:
            print("Äang táº£i model (cÃ³ thá»ƒ máº¥t vÃ i phÃºt)...")
            
            for line in response.iter_lines():
                if line:
                    try:
                        data = json.loads(line.decode('utf-8'))
                        if 'status' in data:
                            print(f"Status: {data['status']}")
                        if 'completed_at' in data:
                            print(f"HoÃ n thÃ nh: {data['completed_at']}")
                            break
                    except json.JSONDecodeError:
                        continue
            
            print(f"âœ… Model {model_name} Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t thÃ nh cÃ´ng!")
            return True
        else:
            print(f"âŒ Lá»—i khi cÃ i Ä‘áº·t model: {response.status_code}")
            return False
            
    except Exception as e:
        print(f"âŒ Lá»—i: {e}")
        return False

def check_system_requirements():
    """Kiá»ƒm tra yÃªu cáº§u há»‡ thá»‘ng"""
    print("ğŸ” Kiá»ƒm tra yÃªu cáº§u há»‡ thá»‘ng...")
    
    # Kiá»ƒm tra RAM
    try:
        import psutil
        ram_gb = psutil.virtual_memory().total / (1024**3)
        print(f"   RAM: {ram_gb:.1f} GB")
        
        if ram_gb < 8:
            print("   âš ï¸  Cáº£nh bÃ¡o: RAM < 8GB cÃ³ thá»ƒ gÃ¢y cháº­m khi cháº¡y gemma3n:e4b")
        else:
            print("   âœ… RAM Ä‘á»§ Ä‘á»ƒ cháº¡y cáº£ 2 models")
    except ImportError:
        print("   â„¹ï¸  KhÃ´ng thá»ƒ kiá»ƒm tra RAM (cáº§n psutil)")
    
    # Kiá»ƒm tra disk space
    try:
        disk_usage = psutil.disk_usage('/')
        disk_gb = disk_usage.free / (1024**3)
        print(f"   Disk space: {disk_gb:.1f} GB free")
        
        if disk_gb < 10:
            print("   âš ï¸  Cáº£nh bÃ¡o: Disk space < 10GB cÃ³ thá»ƒ khÃ´ng Ä‘á»§")
        else:
            print("   âœ… Disk space Ä‘á»§")
    except:
        print("   â„¹ï¸  KhÃ´ng thá»ƒ kiá»ƒm tra disk space")

def main():
    """Main function"""
    print("ğŸš€ Bixby Chatbot - Setup Dual Models")
    print("=" * 50)
    
    # Kiá»ƒm tra yÃªu cáº§u há»‡ thá»‘ng
    check_system_requirements()
    print()
    
    # Kiá»ƒm tra Ollama server
    print("1. Kiá»ƒm tra Ollama server...")
    if not check_ollama_server():
        print("âŒ Ollama server khÃ´ng cháº¡y!")
        print("HÃ£y khá»Ÿi Ä‘á»™ng Ollama trÆ°á»›c:")
        print("  - macOS: brew install ollama && ollama serve")
        print("  - Linux: curl -fsSL https://ollama.ai/install.sh | sh && ollama serve")
        print("  - Windows: Táº£i tá»« https://ollama.ai/download")
        sys.exit(1)
    
    print("âœ… Ollama server Ä‘ang cháº¡y")
    
    # Kiá»ƒm tra models Ä‘Ã£ cÃ i Ä‘áº·t
    print("\n2. Kiá»ƒm tra models Ä‘Ã£ cÃ i Ä‘áº·t...")
    installed_models = get_installed_models()
    print(f"Models hiá»‡n táº¡i: {', '.join(installed_models) if installed_models else 'KhÃ´ng cÃ³'}")
    
    # Models cáº§n cÃ i Ä‘áº·t
    target_models = ["gemma3n:e4b", "gemma2:2b"]
    models_to_install = []
    
    for model in target_models:
        if model in installed_models:
            print(f"âœ… Model {model} Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t!")
        else:
            models_to_install.append(model)
    
    if not models_to_install:
        print("\nğŸ‰ Táº¥t cáº£ models Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t!")
        return
    
    # CÃ i Ä‘áº·t models cÃ²n thiáº¿u
    print(f"\n3. CÃ i Ä‘áº·t {len(models_to_install)} models cÃ²n thiáº¿u...")
    
    for i, model in enumerate(models_to_install, 1):
        print(f"\n[{i}/{len(models_to_install)}] CÃ i Ä‘áº·t {model}...")
        
        if model == "gemma3n:e4b":
            print("   â„¹ï¸  Model nÃ y cáº§n nhiá»u RAM (khuyáº¿n nghá»‹ 8GB+)")
            print("   â„¹ï¸  KÃ­ch thÆ°á»›c: ~4GB")
        elif model == "gemma2:2b":
            print("   â„¹ï¸  Model nháº¹, phÃ¹ há»£p vá»›i server yáº¿u")
            print("   â„¹ï¸  KÃ­ch thÆ°á»›c: ~1.5GB")
        
        if install_model(model):
            print(f"   âœ… {model} cÃ i Ä‘áº·t thÃ nh cÃ´ng!")
        else:
            print(f"   âŒ {model} cÃ i Ä‘áº·t tháº¥t báº¡i!")
            choice = input("   Báº¡n cÃ³ muá»‘n tiáº¿p tá»¥c vá»›i model khÃ¡c khÃ´ng? (y/n): ")
            if choice.lower() != 'y':
                sys.exit(1)
    
    print("\nğŸ‰ HoÃ n thÃ nh! Models Ä‘Ã£ sáºµn sÃ ng sá»­ dá»¥ng.")
    print("\nğŸ“‹ ThÃ´ng tin cáº¥u hÃ¬nh:")
    print("   - Primary model: gemma3n:e4b (máº¡nh hÆ¡n, cáº§n nhiá»u RAM)")
    print("   - Fallback model: gemma2:2b (nháº¹ hÆ¡n, phÃ¹ há»£p server yáº¿u)")
    print("   - Há»‡ thá»‘ng sáº½ tá»± Ä‘á»™ng chuyá»ƒn Ä‘á»•i náº¿u model chÃ­nh khÃ´ng kháº£ dá»¥ng")
    
    print("\nğŸš€ Äá»ƒ khá»Ÿi Ä‘á»™ng chatbot:")
    print("  cd ChatBot/backend")
    print("  python main.py")

if __name__ == "__main__":
    main()
