#!/usr/bin/env python3
"""
Enhanced Model Manager - Qu·∫£n l√Ω t·∫•t c·∫£ models ƒë·ªôc l·∫≠p
H·ªó tr·ª£ auto-download, auto-detection, v√† production deployment
"""

import os
import sys
import json
import time
import requests
import subprocess
import logging
import shutil
from pathlib import Path
from typing import List, Dict, Optional, Set, Any
from enum import Enum
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed

logger = logging.getLogger(__name__)

class ModelType(Enum):
    """C√°c lo·∫°i model ƒë∆∞·ª£c h·ªó tr·ª£"""
    OLLAMA = "ollama"
    VOSK = "vosk"
    GITHUB = "github"
    LOCAL = "local"

@dataclass
class ModelInfo:
    """Th√¥ng tin model"""
    name: str
    type: ModelType
    path: str
    size_mb: float
    status: str  # "available", "downloading", "error", "not_found"
    description: str
    download_url: Optional[str] = None
    required: bool = False

class ModelManager:
    """Enhanced Model Manager v·ªõi auto-detection v√† smart management"""
    
    def __init__(self, base_dir: str = "/app"):
        self.base_dir = Path(base_dir)
        self.data_dir = self.base_dir / "data"
        self.models_dir = self.data_dir / "models"
        self.logs_dir = self.base_dir / "logs"
        
        # T·∫°o th∆∞ m·ª•c c·∫ßn thi·∫øt
        self.data_dir.mkdir(exist_ok=True)
        self.models_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)
        
        # Cache cho models ƒë√£ detect
        self._detected_models: Dict[str, ModelInfo] = {}
        self._model_status: Dict[str, str] = {}
        
        # Configuration
        self.config = self._load_config()
        
    def _load_config(self) -> Dict:
        """Load configuration t·ª´ config.json"""
        config_path = self.base_dir / "config.json"
        if config_path.exists():
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.error(f"Failed to load config: {e}")
        
        # Default config
        return {
            "OLLAMA_BASE_URL": "http://localhost:11434",
            "OLLAMA_MODEL": "gemma2:9b",
            "VOSK_MODEL_PATH": "/app/data/models/vosk-vi",
            "PREFERRED_AI_PROVIDER": "ollama",
            "AUTO_DOWNLOAD_MODELS": True,
            "MODEL_DOWNLOAD_TIMEOUT": 1800,
            "MAX_CONCURRENT_DOWNLOADS": 2
        }
    
    def detect_all_models(self) -> Dict[str, ModelInfo]:
        """Detect t·∫•t c·∫£ models c√≥ s·∫µn tr√™n h·ªá th·ªëng"""
        logger.info("üîç Detecting all available models...")
        
        detected_models = {}
        
        # Detect Ollama models
        ollama_models = self._detect_ollama_models()
        detected_models.update(ollama_models)
        
        # Detect Vosk models
        vosk_models = self._detect_vosk_models()
        detected_models.update(vosk_models)
        
        # Detect local models
        local_models = self._detect_local_models()
        detected_models.update(local_models)
        
        # Detect GitHub models (n·∫øu c√≥ API key)
        if self.config.get("API_KEY"):
            github_models = self._detect_github_models()
            detected_models.update(github_models)
        
        self._detected_models = detected_models
        logger.info(f"‚úÖ Detected {len(detected_models)} models")
        
        return detected_models
    
    def _detect_ollama_models(self) -> Dict[str, ModelInfo]:
        """Detect Ollama models"""
        models = {}
        
        try:
            # Ki·ªÉm tra Ollama server
            base_url = self.config.get("OLLAMA_BASE_URL", "http://localhost:11434")
            response = requests.get(f"{base_url}/api/tags", timeout=5)
            
            if response.status_code == 200:
                data = response.json()
                for model in data.get('models', []):
                    model_name = model.get('name', '')
                    model_info = ModelInfo(
                        name=model_name,
                        type=ModelType.OLLAMA,
                        path=f"ollama://{model_name}",
                        size_mb=model.get('size', 0) / (1024*1024),
                        status="available",
                        description=f"Ollama model: {model_name}",
                        required=(model_name == self.config.get("OLLAMA_MODEL"))
                    )
                    models[f"ollama:{model_name}"] = model_info
                    
                logger.info(f"Found {len(models)} Ollama models")
            else:
                logger.warning("Ollama server not responding")
                
        except Exception as e:
            logger.debug(f"Ollama detection failed: {e}")
        
        return models
    
    def _detect_vosk_models(self) -> Dict[str, ModelInfo]:
        """Detect Vosk models"""
        models = {}
        
        vosk_models = {
            "vi": {
                "path": "vosk-vi",
                "description": "Vietnamese Vosk model",
                "size_mb": 78,
                "url": "https://alphacephei.com/vosk/models/vosk-model-vn-0.4.zip"
            },
            "en": {
                "path": "vosk-en", 
                "description": "English Vosk model",
                "size_mb": 40,
                "url": "https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip"
            }
        }
        
        for lang, info in vosk_models.items():
            model_path = self.models_dir / info["path"]
            status = "available" if model_path.exists() and any(model_path.iterdir()) else "not_found"
            
            model_info = ModelInfo(
                name=f"vosk-{lang}",
                type=ModelType.VOSK,
                path=str(model_path),
                size_mb=info["size_mb"],
                status=status,
                description=info["description"],
                download_url=info["url"],
                required=(lang == "vi")  # Vietnamese l√† required
            )
            models[f"vosk:{lang}"] = model_info
        
        return models
    
    def _detect_local_models(self) -> Dict[str, ModelInfo]:
        """Detect local models trong th∆∞ m·ª•c models"""
        models = {}
        
        # Scan th∆∞ m·ª•c models cho c√°c model t√πy ch·ªânh
        for model_dir in self.models_dir.glob("*"):
            if model_dir.is_dir() and not model_dir.name.startswith("vosk-"):
                # Ki·ªÉm tra xem c√≥ ph·∫£i l√† model kh√¥ng
                if self._is_valid_model_directory(model_dir):
                    model_info = ModelInfo(
                        name=model_dir.name,
                        type=ModelType.LOCAL,
                        path=str(model_dir),
                        size_mb=self._get_directory_size_mb(model_dir),
                        status="available",
                        description=f"Local model: {model_dir.name}"
                    )
                    models[f"local:{model_dir.name}"] = model_info
        
        return models
    
    def _detect_github_models(self) -> Dict[str, ModelInfo]:
        """Detect GitHub models (n·∫øu c√≥ API key)"""
        models = {}
        
        if self.config.get("API_KEY"):
            model_name = self.config.get("MODEL", "openai/gpt-4o-mini")
            model_info = ModelInfo(
                name=model_name,
                type=ModelType.GITHUB,
                path=f"github://{model_name}",
                size_mb=0,  # Unknown size
                status="available",
                description=f"GitHub AI model: {model_name}",
                required=True
            )
            models[f"github:{model_name}"] = model_info
        
        return models
    
    def _is_valid_model_directory(self, model_dir: Path) -> bool:
        """Ki·ªÉm tra xem th∆∞ m·ª•c c√≥ ph·∫£i l√† model h·ª£p l·ªá kh√¥ng"""
        # Ki·ªÉm tra c√°c file c·∫ßn thi·∫øt cho model
        required_files = ["config.json", "model.bin", "tokenizer.json"]
        existing_files = [f.name for f in model_dir.iterdir() if f.is_file()]
        
        # C·∫ßn c√≥ √≠t nh·∫•t 2 file trong s·ªë required files
        return len(set(required_files) & set(existing_files)) >= 2
    
    def _get_directory_size_mb(self, directory: Path) -> float:
        """T√≠nh k√≠ch th∆∞·ªõc th∆∞ m·ª•c theo MB"""
        try:
            total_size = sum(f.stat().st_size for f in directory.rglob('*') if f.is_file())
            return total_size / (1024 * 1024)
        except Exception:
            return 0.0
    
    def ensure_required_models(self) -> Dict[str, bool]:
        """ƒê·∫£m b·∫£o t·∫•t c·∫£ required models c√≥ s·∫µn"""
        logger.info("üîß Ensuring required models...")
        
        # Detect models tr∆∞·ªõc
        all_models = self.detect_all_models()
        
        # T√¨m required models
        required_models = {k: v for k, v in all_models.items() if v.required}
        missing_models = {k: v for k, v in required_models.items() if v.status != "available"}
        
        results = {}
        
        if not missing_models:
            logger.info("‚úÖ All required models are available")
            return {k: True for k in required_models.keys()}
        
        # Download missing models
        logger.info(f"üì• Downloading {len(missing_models)} missing models...")
        
        with ThreadPoolExecutor(max_workers=self.config.get("MAX_CONCURRENT_DOWNLOADS", 2)) as executor:
            future_to_model = {
                executor.submit(self._download_model, model_key, model_info): model_key
                for model_key, model_info in missing_models.items()
            }
            
            for future in as_completed(future_to_model):
                model_key = future_to_model[future]
                try:
                    success = future.result()
                    results[model_key] = success
                    status = "‚úÖ" if success else "‚ùå"
                    logger.info(f"{status} {model_key}: {'Downloaded' if success else 'Failed'}")
                except Exception as e:
                    results[model_key] = False
                    logger.error(f"‚ùå {model_key}: Download error - {e}")
        
        return results
    
    def _download_model(self, model_key: str, model_info: ModelInfo) -> bool:
        """Download m·ªôt model c·ª• th·ªÉ"""
        logger.info(f"üì• Downloading {model_key}...")
        
        try:
            if model_info.type == ModelType.OLLAMA:
                return self._download_ollama_model(model_info.name)
            elif model_info.type == ModelType.VOSK:
                return self._download_vosk_model(model_key, model_info)
            elif model_info.type == ModelType.LOCAL:
                return self._download_local_model(model_key, model_info)
            else:
                logger.warning(f"Unknown model type: {model_info.type}")
                return False
                
        except Exception as e:
            logger.error(f"Download failed for {model_key}: {e}")
            return False
    
    def _download_ollama_model(self, model_name: str) -> bool:
        """Download Ollama model"""
        try:
            timeout = self.config.get("MODEL_DOWNLOAD_TIMEOUT", 1800)
            result = subprocess.run(
                ['ollama', 'pull', model_name],
                timeout=timeout,
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                logger.info(f"‚úÖ Ollama model {model_name} downloaded successfully")
                return True
            else:
                logger.error(f"‚ùå Failed to download Ollama model {model_name}: {result.stderr}")
                return False
                
        except subprocess.TimeoutExpired:
            logger.error(f"‚ùå Download timeout for Ollama model {model_name}")
            return False
        except Exception as e:
            logger.error(f"‚ùå Error downloading Ollama model {model_name}: {e}")
            return False
    
    def _download_vosk_model(self, model_key: str, model_info: ModelInfo) -> bool:
        """Download Vosk model"""
        try:
            import zipfile
            
            # T·∫°o th∆∞ m·ª•c t·∫°m
            temp_dir = self.models_dir / "temp"
            temp_dir.mkdir(exist_ok=True)
            
            # Download file
            zip_path = temp_dir / f"{model_info.name}.zip"
            
            response = requests.get(model_info.download_url, stream=True, timeout=60)
            response.raise_for_status()
            
            with open(zip_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
            
            # Extract
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            
            # Move to target location
            target_dir = Path(model_info.path)
            if target_dir.exists():
                shutil.rmtree(target_dir)
            
            # Find extracted directory
            extracted_dirs = [d for d in temp_dir.iterdir() if d.is_dir()]
            if extracted_dirs:
                shutil.move(str(extracted_dirs[0]), str(target_dir))
            
            # Cleanup
            zip_path.unlink()
            if temp_dir.exists():
                shutil.rmtree(temp_dir)
            
            logger.info(f"‚úÖ Vosk model {model_key} downloaded successfully")
            return True
            
        except Exception as e:
            logger.error(f"‚ùå Failed to download Vosk model {model_key}: {e}")
            return False
    
    def _download_local_model(self, model_key: str, model_info: ModelInfo) -> bool:
        """Download local model (placeholder)"""
        logger.warning(f"Local model download not implemented for {model_key}")
        return False
    
    def get_model_status(self) -> Dict[str, Any]:
        """L·∫•y status c·ªßa t·∫•t c·∫£ models"""
        all_models = self.detect_all_models()
        
        status = {
            "total_models": len(all_models),
            "available_models": len([m for m in all_models.values() if m.status == "available"]),
            "required_models": len([m for m in all_models.values() if m.required]),
            "missing_required": len([m for m in all_models.values() if m.required and m.status != "available"]),
            "models_by_type": {},
            "model_details": {}
        }
        
        # Group by type
        for model_key, model_info in all_models.items():
            model_type = model_info.type.value
            if model_type not in status["models_by_type"]:
                status["models_by_type"][model_type] = []
            status["models_by_type"][model_type].append({
                "name": model_info.name,
                "status": model_info.status,
                "size_mb": model_info.size_mb,
                "required": model_info.required
            })
            
            # Detailed info
            status["model_details"][model_key] = {
                "name": model_info.name,
                "type": model_info.type.value,
                "path": model_info.path,
                "size_mb": model_info.size_mb,
                "status": model_info.status,
                "description": model_info.description,
                "required": model_info.required
            }
        
        return status
    
    def cleanup_old_models(self, keep_recent: int = 3) -> Dict[str, bool]:
        """D·ªçn d·∫πp models c≈© (gi·ªØ l·∫°i keep_recent models m·ªõi nh·∫•t)"""
        logger.info(f"üßπ Cleaning up old models (keeping {keep_recent} recent)...")
        
        results = {}
        
        # Ch·ªâ cleanup Ollama models
        ollama_models = {k: v for k, v in self._detected_models.items() if v.type == ModelType.OLLAMA}
        
        if len(ollama_models) <= keep_recent:
            logger.info("No cleanup needed")
            return results
        
        # Sort by modification time (n·∫øu c√≥)
        sorted_models = sorted(ollama_models.items(), key=lambda x: x[1].name)
        
        # Remove old models (tr·ª´ required v√† recent)
        for i, (model_key, model_info) in enumerate(sorted_models):
            if i < len(sorted_models) - keep_recent and not model_info.required:
                try:
                    result = subprocess.run(['ollama', 'rm', model_info.name], capture_output=True, text=True)
                    if result.returncode == 0:
                        results[model_key] = True
                        logger.info(f"‚úÖ Removed old model: {model_info.name}")
                    else:
                        results[model_key] = False
                        logger.warning(f"‚ö†Ô∏è Failed to remove model: {model_info.name}")
                except Exception as e:
                    results[model_key] = False
                    logger.error(f"‚ùå Error removing model {model_info.name}: {e}")
        
        return results
    
    def validate_models(self) -> Dict[str, bool]:
        """Validate t·∫•t c·∫£ models"""
        logger.info("üîç Validating models...")
        
        all_models = self.detect_all_models()
        results = {}
        
        for model_key, model_info in all_models.items():
            if model_info.status != "available":
                results[model_key] = False
                continue
            
            try:
                if model_info.type == ModelType.OLLAMA:
                    results[model_key] = self._validate_ollama_model(model_info)
                elif model_info.type == ModelType.VOSK:
                    results[model_key] = self._validate_vosk_model(model_info)
                elif model_info.type == ModelType.GITHUB:
                    results[model_key] = self._validate_github_model(model_info)
                else:
                    results[model_key] = True  # Assume local models are valid
                    
            except Exception as e:
                logger.error(f"Validation error for {model_key}: {e}")
                results[model_key] = False
        
        return results
    
    def _validate_ollama_model(self, model_info: ModelInfo) -> bool:
        """Validate Ollama model"""
        try:
            payload = {
                "model": model_info.name,
                "prompt": "Test",
                "stream": False
            }
            
            base_url = self.config.get("OLLAMA_BASE_URL", "http://localhost:11434")
            response = requests.post(
                f"{base_url}/api/generate",
                json=payload,
                timeout=30
            )
            
            return response.status_code == 200
            
        except Exception as e:
            logger.debug(f"Ollama validation failed for {model_info.name}: {e}")
            return False
    
    def _validate_vosk_model(self, model_info: ModelInfo) -> bool:
        """Validate Vosk model"""
        try:
            model_path = Path(model_info.path)
            required_files = ["am", "conf", "graph", "ivector", "rescoring", "rnnlm"]
            
            # Ki·ªÉm tra th∆∞ m·ª•c v√† files c·∫ßn thi·∫øt
            if not model_path.exists():
                return False
            
            # Ki·ªÉm tra √≠t nh·∫•t m·ªôt s·ªë file quan tr·ªçng
            existing_dirs = [d.name for d in model_path.iterdir() if d.is_dir()]
            return len(set(required_files) & set(existing_dirs)) >= 3
            
        except Exception as e:
            logger.debug(f"Vosk validation failed for {model_info.name}: {e}")
            return False
    
    def _validate_github_model(self, model_info: ModelInfo) -> bool:
        """Validate GitHub model (ki·ªÉm tra API key)"""
        return bool(self.config.get("API_KEY"))

# Global model manager instance
model_manager = ModelManager()
