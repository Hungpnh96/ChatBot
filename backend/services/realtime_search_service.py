# services/realtime_search_service.py - FIXED VERSION v·ªõi better search

import logging
import requests
import asyncio
import re
from typing import Optional, Dict, Any, List
from datetime import datetime, timedelta
import json
import hashlib
from bs4 import BeautifulSoup
import urllib.parse
from config.settings import settings

logger = logging.getLogger(__name__)

class RealtimeSearchService:
    """FIXED Realtime Search Service v·ªõi improved search accuracy"""
    
    def __init__(self):
        self.cache = {}
        self.cache_ttl = 1800  # Gi·∫£m xu·ªëng 30 ph√∫t ƒë·ªÉ c·∫≠p nh·∫≠t th∆∞·ªùng xuy√™n h∆°n
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        
    def _should_search(self, user_query: str) -> bool:
        """Ph√¢n t√≠ch xem c√¢u h·ªèi c√≥ c·∫ßn search th√¥ng tin m·ªõi kh√¥ng"""
        
        # ENHANCED keywords cho th√¥ng tin th·ªùi gian th·ª±c
        realtime_keywords = [
            # Th·ªùi gian - QUAN TR·ªåNG
            "hi·ªán t·∫°i", "b√¢y gi·ªù", "h√¥m nay", "nƒÉm nay", "th√°ng n√†y", "2024", "2025",
            "m·ªõi nh·∫•t", "g·∫ßn ƒë√¢y", "v·ª´a", "latest", "current", "now", "today",
            "ƒë∆∞∆°ng nhi·ªám", "incumbent", "present",
            
            # Ch√≠nh tr·ªã - l√£nh ƒë·∫°o - QUAN TR·ªåNG  
            "ch·ªß t·ªãch n∆∞·ªõc", "th·ªß t∆∞·ªõng", "t·ªïng th·ªëng", "th·ªß t∆∞·ªõng ch√≠nh ph·ªß",
            "president", "prime minister", "ceo", "gi√°m ƒë·ªëc", "l√£nh ƒë·∫°o",
            "ƒë·∫£ng tr∆∞·ªüng", "t·ªïng b√≠ th∆∞",
            
            # Kinh t·∫ø - t√†i ch√≠nh
            "gi√°", "t·ª∑ gi√°", "ch·ª©ng kho√°n", "v√†ng", "bitcoin", "usd", "vnƒë",
            "price", "stock", "exchange rate", "market", "th·ªã tr∆∞·ªùng",
            
            # Th·ªùi ti·∫øt
            "th·ªùi ti·∫øt", "nhi·ªát ƒë·ªô", "m∆∞a", "n·∫Øng", "weather", "temperature",
            "d·ª± b√°o th·ªùi ti·∫øt", "forecast",
            
            # Tin t·ª©c - s·ª± ki·ªán
            "tin t·ª©c", "s·ª± ki·ªán", "news", "breaking", "b√°o", "th√¥ng tin m·ªõi",
            "x·∫£y ra", "di·ªÖn ra", "v·ª´a x·∫£y ra", "c·∫≠p nh·∫≠t",
            
            # Th·ªÉ thao
            "b√≥ng ƒë√°", "world cup", "k·∫øt qu·∫£", "tr·∫≠n ƒë·∫•u", "t·ª∑ s·ªë", "euro",
            "championship", "football", "soccer",
            
            # C√¥ng ngh·ªá
            "iphone m·ªõi", "samsung", "update", "phi√™n b·∫£n m·ªõi", "ra m·∫Øt",
            "launch", "release", "tech news"
        ]
        
        query_lower = user_query.lower()
        
        # Ki·ªÉm tra keywords
        for keyword in realtime_keywords:
            if keyword in query_lower:
                logger.info(f"üîç Detected realtime search need: '{keyword}' in query")
                return True
                
        # Patterns cho c√¢u h·ªèi c·∫ßn info m·ªõi
        realtime_patterns = [
            "ai l√†", "who is", "bao nhi√™u", "how much",
            "khi n√†o", "when", "c√≥ g√¨ m·ªõi", "what's new",
            "di·ªÖn bi·∫øn", "t√¨nh h√¨nh", "c·∫≠p nh·∫≠t"
        ]
        
        for pattern in realtime_patterns:
            if pattern in query_lower:
                logger.info(f"üîç Detected realtime pattern: '{pattern}' in query")
                return True
                
        return False
    
    def _get_cache_key(self, query: str, source: str = "") -> str:
        """T·∫°o cache key t·ª´ query"""
        return hashlib.md5(f"{source}_{query}".lower().encode()).hexdigest()
    
    def _is_cache_valid(self, cache_key: str) -> bool:
        """Ki·ªÉm tra cache c√≤n valid kh√¥ng"""
        if cache_key not in self.cache:
            return False
            
        cached_time = self.cache[cache_key].get('timestamp', 0)
        is_valid = (datetime.now().timestamp() - cached_time) < self.cache_ttl
        
        if not is_valid:
            logger.info(f"üóëÔ∏è Cache expired for key: {cache_key[:8]}")
        
        return is_valid
    
    async def search_wikipedia_vietnam(self, query: str) -> List[Dict[str, Any]]:
        """IMPROVED Wikipedia search v·ªõi focus on current info"""
        try:
            cache_key = self._get_cache_key(query, "wikipedia_vn")
            if self._is_cache_valid(cache_key):
                logger.info(f"üì¶ Using cached Wikipedia VN results for: {query}")
                return self.cache[cache_key]['results']
            
            # Wikipedia Vietnamese API v·ªõi multiple search strategies
            search_url = "https://vi.wikipedia.org/w/api.php"
            
            # Strategy 1: Direct search v·ªõi current context
            search_params = {
                'action': 'query',
                'format': 'json',
                'list': 'search',
                'srsearch': f"{query} 2024 2025 hi·ªán t·∫°i",  # Th√™m context m·ªõi
                'srlimit': 5,
                'srprop': 'snippet|timestamp'
            }
            
            logger.info(f"üîç Wikipedia VN search for: {query}")
            
            response = requests.get(search_url, params=search_params, timeout=10)
            response.raise_for_status()
            
            search_data = response.json()
            results = []
            
            for item in search_data.get('query', {}).get('search', []):
                title = item.get('title', '')
                snippet = item.get('snippet', '')
                
                # Clean HTML tags from snippet
                snippet = re.sub(r'<[^>]+>', '', snippet)
                
                # Get detailed page content for first result
                if len(results) == 0:
                    try:
                        page_content = await self._get_wikipedia_page_content(title)
                        if page_content:
                            snippet = page_content
                    except Exception as e:
                        logger.warning(f"Could not get detailed content for {title}: {e}")
                
                page_url = f"https://vi.wikipedia.org/wiki/{urllib.parse.quote(title)}"
                
                results.append({
                    'title': title,
                    'snippet': snippet,
                    'link': page_url,
                    'source': 'Wikipedia (Vi)',
                    'published': 'Recent'
                })
            
            # Cache v·ªõi TTL ng·∫Øn h∆°n cho th√¥ng tin ch√≠nh tr·ªã
            cache_ttl = 900 if any(word in query.lower() for word in ["ch·ªß t·ªãch", "th·ªß t∆∞·ªõng", "president"]) else self.cache_ttl
            
            self.cache[cache_key] = {
                'results': results,
                'timestamp': datetime.now().timestamp() - (self.cache_ttl - cache_ttl)
            }
            
            logger.info(f"‚úÖ Found {len(results)} Wikipedia VN results for: {query}")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå Wikipedia VN search failed: {e}")
            return []
    
    async def _get_wikipedia_page_content(self, title: str) -> str:
        """L·∫•y n·ªôi dung chi ti·∫øt t·ª´ Wikipedia page"""
        try:
            content_url = "https://vi.wikipedia.org/w/api.php"
            content_params = {
                'action': 'query',
                'format': 'json',
                'titles': title,
                'prop': 'extracts',
                'exintro': True,
                'explaintext': True,
                'exsectionformat': 'plain'
            }
            
            response = requests.get(content_url, params=content_params, timeout=8)
            response.raise_for_status()
            
            data = response.json()
            pages = data.get('query', {}).get('pages', {})
            
            for page_id, page_data in pages.items():
                if page_id != '-1':  # Page exists
                    extract = page_data.get('extract', '')
                    if extract:
                        # L·∫•y 500 k√Ω t·ª± ƒë·∫ßu v√† t√¨m th√¥ng tin quan tr·ªçng
                        extract = extract[:500]
                        
                        # T√¨m th√¥ng tin v·ªÅ nƒÉm hi·ªán t·∫°i
                        if '2024' in extract or '2025' in extract:
                            return extract
                        
                        return extract[:300] + "..."
            
            return ""
            
        except Exception as e:
            logger.warning(f"Error getting Wikipedia page content: {e}")
            return ""
    
    async def search_google(self, query: str, num_results: int = 3) -> List[Dict[str, Any]]:
        """IMPROVED Google search with better targeting"""
        try:
            cache_key = self._get_cache_key(query, "google_improved")
            if self._is_cache_valid(cache_key):
                logger.info(f"üì¶ Using cached Google results for: {query}")
                return self.cache[cache_key]['results']
            
            # IMPROVED Google search v·ªõi multiple strategies
            search_queries = [
                f"{query} 2024 2025 site:vi.wikipedia.org",
                f"{query} hi·ªán t·∫°i site:vnexpress.net",
                f"{query} m·ªõi nh·∫•t site:tuoitre.vn",
                f"{query} site:dantri.com.vn"
            ]
            
            all_results = []
            
            for search_query in search_queries[:2]:  # Th·ª≠ 2 query ƒë·∫ßu
                try:
                    encoded_query = urllib.parse.quote_plus(search_query)
                    url = f"https://www.google.com/search?q={encoded_query}&num=3&hl=vi&gl=vn"
                    
                    logger.info(f"üîç Google search: {search_query}")
                    
                    response = requests.get(url, headers=self.headers, timeout=10)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.text, 'html.parser')
                    search_results = soup.find_all('div', class_='g')
                    
                    for result in search_results[:2]:  # Top 2 t·ª´ m·ªói query
                        try:
                            title_elem = result.find('h3')
                            title = title_elem.get_text() if title_elem else ""
                            
                            link_elem = result.find('a')
                            link = link_elem.get('href', '') if link_elem else ""
                            
                            snippet_elem = result.find('span', class_='aCOpRe') or result.find('div', class_='VwiC3b')
                            snippet = snippet_elem.get_text() if snippet_elem else ""
                            
                            source_elem = result.find('cite')
                            source = source_elem.get_text() if source_elem else ""
                            
                            if title and link and snippet:
                                all_results.append({
                                    'title': title,
                                    'snippet': snippet,
                                    'link': link,
                                    'source': source,
                                    'published': 'Recent'
                                })
                                
                        except Exception as e:
                            logger.warning(f"Error parsing Google result: {e}")
                            continue
                    
                    # N·∫øu ƒë√£ c√≥ k·∫øt qu·∫£ t·ªët, break
                    if len(all_results) >= 2:
                        break
                        
                except Exception as e:
                    logger.warning(f"Google search query failed: {e}")
                    continue
            
            # Cache results
            self.cache[cache_key] = {
                'results': all_results[:num_results],
                'timestamp': datetime.now().timestamp()
            }
            
            logger.info(f"‚úÖ Found {len(all_results)} Google results for: {query}")
            return all_results[:num_results]
            
        except Exception as e:
            logger.error(f"‚ùå Google search failed: {e}")
            return []
    
    async def search_bing(self, query: str, num_results: int = 3) -> List[Dict[str, Any]]:
        """DuckDuckGo search (reliable fallback)"""
        try:
            cache_key = self._get_cache_key(query, "duckduckgo")
            if self._is_cache_valid(cache_key):
                logger.info(f"üì¶ Using cached DuckDuckGo results for: {query}")
                return self.cache[cache_key]['results']
            
            # DuckDuckGo Instant Answer API
            url = "https://api.duckduckgo.com/"
            params = {
                'q': f"{query} 2024 2025",  # Add current year context
                'format': 'json',
                'no_html': '1',
                'skip_disambig': '1'
            }
            
            logger.info(f"ü¶Ü DuckDuckGo search for: {query}")
            
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = response.json()
            results = []
            
            # Parse DuckDuckGo response
            if data.get('Abstract'):
                results.append({
                    'title': data.get('Heading', query),
                    'snippet': data.get('Abstract', ''),
                    'link': data.get('AbstractURL', ''),
                    'source': data.get('AbstractSource', 'DuckDuckGo'),
                    'published': 'Recent'
                })
            
            # Related topics
            for topic in data.get('RelatedTopics', [])[:num_results-1]:
                if isinstance(topic, dict) and topic.get('Text'):
                    results.append({
                        'title': topic.get('Text', '').split(' - ')[0],
                        'snippet': topic.get('Text', ''),
                        'link': topic.get('FirstURL', ''),
                        'source': 'DuckDuckGo',
                        'published': 'Recent'
                    })
            
            self.cache[cache_key] = {
                'results': results,
                'timestamp': datetime.now().timestamp()
            }
            
            logger.info(f"‚úÖ Found {len(results)} DuckDuckGo results for: {query}")
            return results
            
        except Exception as e:
            logger.error(f"‚ùå DuckDuckGo search failed: {e}")
            return []
    
    def _extract_key_info(self, search_results: List[Dict[str, Any]], query: str) -> str:
        """IMPROVED info extraction with current context priority"""
        if not search_results:
            return ""
        
        info_parts = []
        current_date = datetime.now().strftime('%d/%m/%Y')
        
        for i, result in enumerate(search_results[:3]):
            title = result.get('title', '')
            snippet = result.get('snippet', '')
            source = result.get('source', '')
            
            if snippet:
                # Prioritize snippets with current years
                if any(year in snippet for year in ['2024', '2025']) or 'hi·ªán t·∫°i' in snippet.lower():
                    info_parts.insert(0, f"[{source}] {snippet}")  # Insert at beginning
                else:
                    info_parts.append(f"[{source}] {snippet}")
            elif title:
                info_parts.append(f"[{source}] {title}")
        
        if info_parts:
            combined_info = "\n".join(info_parts)
            
            search_summary = f"""Th√¥ng tin t√¨m ki·∫øm m·ªõi nh·∫•t v·ªÅ '{query}':

{combined_info}

(Ngu·ªìn: T√¨m ki·∫øm th·ªùi gian th·ª±c ng√†y {current_date})"""
            
            return search_summary
        
        return ""
    
    async def search_and_get_info(self, user_query: str) -> Optional[str]:
        """
        MAIN METHOD: Improved search v·ªõi better accuracy
        """
        try:
            if not self._should_search(user_query):
                logger.info(f"üö´ No realtime search needed for: {user_query}")
                return None
            
            logger.info(f"üöÄ Starting IMPROVED realtime search for: {user_query}")
            
            search_query = self._optimize_search_query(user_query)
            all_results = []
            
            # Strategy 1: Wikipedia Vietnam (highest priority for Vietnamese content)
            logger.info("üìö Searching Wikipedia Vietnam...")
            wiki_results = await self.search_wikipedia_vietnam(search_query)
            all_results.extend(wiki_results)
            
            # Strategy 2: DuckDuckGo (reliable)
            logger.info("ü¶Ü Searching DuckDuckGo...")
            duck_results = await self.search_bing(search_query, 2)
            all_results.extend(duck_results)
            
            # Strategy 3: Google scraping (if not enough results)
            if len(all_results) < 2:
                logger.info("üîç Searching Google (backup)...")
                google_results = await self.search_google(search_query, 2)
                all_results.extend(google_results)
            
            if all_results:
                info = self._extract_key_info(all_results, user_query)
                if info:
                    logger.info(f"‚úÖ FOUND CURRENT INFO: {len(info)} characters")
                    logger.debug(f"Search results preview: {info[:200]}...")
                    return info
            
            logger.warning(f"‚ùå No current search results found for: {user_query}")
            return None
            
        except Exception as e:
            logger.error(f"üí• Search and get info failed: {e}")
            return None
    
    def _optimize_search_query(self, user_query: str) -> str:
        """IMPROVED query optimization with current context"""
        
        # UPDATED optimizations v·ªõi th√¥ng tin 2024-2025
        query_optimizations = {
            "ch·ªß t·ªãch n∆∞·ªõc vi·ªát nam": "L∆∞∆°ng C∆∞·ªùng ch·ªß t·ªãch n∆∞·ªõc Vi·ªát Nam 2024 2025",
            "ch·ªß t·ªãch n∆∞·ªõc vn": "L∆∞∆°ng C∆∞·ªùng ch·ªß t·ªãch n∆∞·ªõc Vi·ªát Nam 2024", 
            "th·ªß t∆∞·ªõng vi·ªát nam": "Ph·∫°m Minh Ch√≠nh th·ªß t∆∞·ªõng Vi·ªát Nam 2024",
            "president vietnam": "L∆∞∆°ng C∆∞·ªùng president Vietnam 2024",
            "gi√° v√†ng": "gi√° v√†ng h√¥m nay Vi·ªát Nam",
            "t·ª∑ gi√° usd": "t·ª∑ gi√° USD VND h√¥m nay",
            "th·ªùi ti·∫øt h√† n·ªôi": "th·ªùi ti·∫øt H√† N·ªôi h√¥m nay",
            "th·ªùi ti·∫øt tp h·ªì ch√≠ minh": "th·ªùi ti·∫øt TP.HCM h√¥m nay",
            "bitcoin": "gi√° Bitcoin h√¥m nay"
        }
        
        query_lower = user_query.lower()
        
        # T√¨m optimization ph√π h·ª£p
        for key, optimized in query_optimizations.items():
            if key in query_lower:
                logger.info(f"üéØ Optimized query: '{user_query}' -> '{optimized}'")
                return optimized
        
        # Th√™m context nƒÉm hi·ªán t·∫°i
        if any(word in query_lower for word in ["hi·ªán t·∫°i", "b√¢y gi·ªù", "ai l√†"]):
            optimized = f"{user_query} 2024 2025"
            logger.info(f"üéØ Added current context: '{user_query}' -> '{optimized}'")
            return optimized
        
        return user_query
    
    def get_search_stats(self) -> Dict[str, Any]:
        """Get search statistics"""
        return {
            "cache_size": len(self.cache),
            "cache_ttl_seconds": self.cache_ttl,
            "search_methods": [
                "Wikipedia Vietnam (Priority)",
                "DuckDuckGo (Reliable)", 
                "Google Scraping (Backup)"
            ],
            "cost": "FREE",
            "improvements": [
                "Shorter cache TTL for political info",
                "Multiple search strategies",
                "Current year context addition",
                "Detailed Wikipedia content extraction"
            ]
        }

# Global instance
realtime_search_service = RealtimeSearchService()